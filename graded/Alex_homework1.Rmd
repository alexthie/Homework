---
title: 'Bios 6301: Assignment 1'
author: "Alexander Thiemicke"
date: "2015-10-08"
#"'r format(Sys.time(), '%B, %d, %Y')"
output: pdf_document
# toc: true
#  fig_caption: yes
#  md_extensions: =raw_tex
#header-includes:
#  - \usepackage{fancyhdr}
#  - \usepackage{graphicx}
# - \usepackage{needspace, setspace, relsize, url}
---


50 points total.



### Create a Data Set

A data set in R is called a data.frame.  This particular data set is
made of three categorical variables, or factors: `gender`, `smoker`,
and `exercise`.  In addition `exercise` is an ordered factor.  `age`
and `los` (length of stay) are continuous variables.

```{r}
gender <- c('M','M','F','M','F','F','M','F','M')
age <- c(34, 64, 38, 63, 40, 73, 27, 51, 47)
smoker <- c('no','yes','no','no','yes','no','no','no','yes')
exercise <- factor(c('moderate','frequent','some','some','moderate','none','none','moderate','moderate'),
                    levels=c('none','some','moderate','frequent'), ordered=TRUE
)
los <- c(4,8,1,10,6,3,9,4,8)
x <- data.frame(gender, age, smoker, exercise, los)
x
```

### Create a Model

We can create a model using our data set.  In this case I’d like to
estimate the association between `los` and all remaining variables.
This means `los` is our dependent variable. The other columns will be
terms in our model.

The `lm` function will take two arguments, a formula and a data set.
The formula is split into two parts, where the vector to the left of
`~` is the dependent variable, and items on the right are terms.

```{r}
lm(los ~ gender + age + smoker + exercise, dat=x)
```

1. Looking at the output, which coefficient seems to have the highest
effect on `los`? (2 points)

```{r}
gender

```


This can be tough because it also depends on the scale of the
variable.  If all the variables are standardized, then this is not
the case.

Given that we only have nine observations, it's not really a good idea
to include all of our variables in the model.  In this case we'd be
"over-fitting" our data.  Let's only include one term, `gender`.

#### Warning

When choosing terms for a model, use prior research, don't just
select the variable with the highest coefficient.

2. Create a model using `los` and `gender` and assign it to the
variable `mod`.  Run the `summary` function with `mod` as its
argument. (5 points)

```{r}
mod <- lm(los ~ gender, dat=x)
summary(mod)
```
Call:
lm(formula = los ~ gender, data = x)

Residuals:
   Min     1Q Median     3Q    Max 
  -3.8   -0.5    0.2    1.2    2.5 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)    3.500      1.099   3.186   0.0154 *
genderM        4.300      1.474   2.917   0.0224 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.197 on 7 degrees of freedom
Multiple R-squared:  0.5487,	Adjusted R-squared:  0.4842 
F-statistic:  8.51 on 1 and 7 DF,  p-value: 0.02243

The summary of our model reports the parameter estimates along with
standard errors, test statistics, and p-values.  This table of
estimates can be extracted with the `coef` function.

### Estimates

3. What is the estimate for the intercept?  What is the estimate for
gender?  Use the `coef` function. (3 points)

```{r}
coef(summary(mod))
```
Estimate Std. Error  t value   Pr(>|t|)
(Intercept)      3.5   1.098701 3.185581 0.01537082
genderM          4.3   1.474061 2.917110 0.02243214
The estimate for intercept is 3.5. The estimate for gender is 4.3.

4. The second column of `coef` are standard errors.  These can be
calculated by taking the `sqrt` of the `diag` of the `vcov` of the
`summary` of `mod`.  Calculate the standard errors. (3 points)

```{r}
sqrt(diag(vcov(summary(mod))))
```
standard errors:
(Intercept)     genderM 
   1.098701    1.474061 

The third column of `coef` are test statistics.  These can be
calculated by dividing the first column by the second column.

```{r}
mod <- lm(los ~ gender, dat=x)
mod.c <- coef(summary(mod))
mod.c[,1]/mod.c[,2]
```

The fourth column of `coef` are p values.  This captures the
probability of observing a more extreme test statistic.  These can be
calculated with the `pt` function, but you will need the
degrees-of-freedom.  For this model, there are 7 degrees-of-freedom.

5. Use the `pt` function to calculate the p value for gender.  The first
argument should be the test statistic for gender.  The second argument
is the degrees-of-freedom.  Also, set the `lower.tail` argument to
FALSE.  Finally multiple this result by two. (4 points)

```{r}
pt(2.91711, 7, lower.tail = FALSE, log.p = FALSE) *2
```
[1] 0.02243216
### Predicted Values

The estimates can be used to create predicted values.

```{r}
3.5+(x$gender=='M')*4.3
```

6. It is even easier to see the predicted values by passing the model
`mod` to the `predict` or `fitted` functions.  Try it out. (2 points)

```{r}
predict(mod)
fitted(mod)
```
> predict(mod)
  1   2   3   4   5   6   7   8   9 
7.8 7.8 3.5 7.8 3.5 3.5 7.8 3.5 7.8 
> fitted(mod)
  1   2   3   4   5   6   7   8   9 
7.8 7.8 3.5 7.8 3.5 3.5 7.8 3.5 7.8 

7. `predict` can also use a new data set.  Pass `newdat` as the second
argument to `predict`. (3 points)

```{r}
newdat <- data.frame(gender=c('F','M','F'))
predict(mod, newdat)
```
  1   2   3 
3.5 7.8 3.5 
### Residuals

The difference between predicted values and observed values are
residuals.

8. Use one of the methods to generate predicted values.  Subtract the
predicted value from the `x$los` column. (5 points)

```{r}
x$los - predict(mod)
```
   1    2    3    4    5    6    7    8    9 
-3.8  0.2 -2.5  2.2  2.5 -0.5  1.2  0.5  0.2 

9. Try passing `mod` to the `residuals` function. (2 points)

```{r}
residuals(mod)
```
   1    2    3    4    5    6    7    8    9 
-3.8  0.2 -2.5  2.2  2.5 -0.5  1.2  0.5  0.2 

10. Square the residuals, and then sum these values.  Compare this to the
result of passing `mod` to the `deviance` function. (6 points)

```{r}
sum(residuals(mod)^2) 
deviance(mod)
```
> sum(residuals(mod)^2) 
[1] 33.8
> deviance(mod)
[1] 33.8
The results are identical.

Remember that our model object has two items in the formula, `los`
and `gender`.  The residual degrees-of-freedom is the number of
observations minus the number of items to account for in the model
formula.

This can be seen by passing `mod` to the function `df.residual`.

```{r}
df.residual(mod)
```
[1] 7
11. Calculate standard error by dividing the deviance by the
degrees-of-freedom, and then taking the square root.  Verify that this
matches the output labeled "Residual standard error" from
`summary(mod)`. (5 points)

```{r}
StdEr = sqrt(deviance(mod)/df.residual(mod))
StdEr
```
[1] 2.197401
Value matches "Residual standard error" output from 'summary(mod)'

Note it will also match this output:

```{r}
predict(mod, se.fit=TRUE)$residual.scale
```

### T-test

Let's compare the results of our model to a two-sample t-test.  We
will compare `los` by men and women.

12. Create a subset of `x` by taking all records where gender is 'M'
and assigning it to the variable `men`.  Do the same for the variable
`women`. (4 points)

```{r}
men <- subset(x, gender=="M", select= gender:los)
women <- subset(x, gender=="F", select= gender:los)
```


13. By default a two-sampled t-test assumes that the two groups have
unequal variances.  You can calculate variance with the `var`
function.  Calculate variance for `los` for the `men` and `women` data
sets. (3 points)

```{r}
losm = men$los
losf = women$los

var (losm)
var(losf)
```
> var (losm)
[1] 5.2
> var(losf)
[1] 4.333333

14. Call the `t.test` function, where the first argument is `los` for
women and the second argument is `los` for men.  Call it a second time
by adding the argument `var.equal` and setting it to TRUE.  Does
either produce output that matches the p value for gender from the
model summary? (3 points)

```{r}
t.test(losm, losf)

t.test(losm, losf, var.equal = TRUE)
```
> t.test(losm, losf)

	Welch Two Sample t-test

data:  losm and losf
t = 2.9509, df = 6.8146, p-value = 0.02205
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 0.8352514 7.7647486
sample estimates:
mean of x mean of y 
      7.8       3.5 


> t.test(losm, losf, var.equal = TRUE)

	Two Sample t-test

data:  losm and losf
t = 2.9171, df = 7, p-value = 0.02243
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 0.8143986 7.7856014
sample estimates:
mean of x mean of y 
      7.8       3.5 
      
The second option matches the output from the model summary.


An alternative way to call `t.test` is to use a formula.

```{r}
t.test(los ~ gender, dat=x, var.equal=TRUE)
# compare p-values
t.test(los ~ gender, dat=x, var.equal=TRUE)$p.value
coef(summary(lm(los ~ gender, dat=x)))[2,4]
```

### Notes/Grade
```{r}
# Another way to do (more reproducible) Question #5:
# pt((mod.c[2,1]/mod.c[2,2]), 7, lower.tail=FALSE)*2
```

Grade: 50/50 points

